{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265015d0",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "Some students copy this file into their assignment repo. DON'T!!! \n",
    "\n",
    "Why? This file is just exercises that help you build that file. Here, we are solving little problems you'll have as you write it. \n",
    "\n",
    "When you create `build_sample.ipynb`, do it from scratch, put the psuedocode structure in place, and proceed from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7d26-85de-44f3-83ac-e98340ac2d0f",
   "metadata": {},
   "source": [
    "## First\n",
    "\n",
    "- Copy `NEAR_regex.py` into the same folder as this file. [It's here](https://ledatascifi.github.io/ledatascifi-2024/content/04/02d_RegexApplication.html#demo) (click the \"+\") or in the community codebook. You should name the file `NEAR_regex.py` and not `NEAR_regex.ipynb`.\n",
    "- Also copy the 10k_files_practice.zip file there into this folder\n",
    "- Make a `.gitignore` file in this folder with  `**10k_files/*` in it.\n",
    "- Copy [this file](https://github.com/donbowen/Class-Notes-1045/raw/main/Midterm%20sandbox/10k_files.zip) into the `10k_files/` folder here.\n",
    "- Copy the things in the assignment's input folder in to the inputs folder here.\n",
    "- Optional: You can install `tqdm` (If you don't, then remove it from the code below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908e0c86-ca84-47f8-95e7-e34c024ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex  # copy this file into the folder this script is in\n",
    "from tqdm import tqdm  # progress bar on loops\n",
    "\n",
    "# if you have tqdm issues, run this in terminal or with ! trick\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#\n",
    "# if that fails, you can disable it\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fec2c-a2bd-4513-9430-33fd2401cdec",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c226f-b543-4c03-a3d6-0dc1c77c3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - load the ML negative words into a list called BHR_negative\n",
    "# BHR is the author names on that paper\n",
    "# \"ML\" might be a better name, but having \"LM\" and \"ML\" in bound\n",
    "# to cause transcription errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ada34b-0a53-4101-ba01-c7701948300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - load the ML positive words into a list called BHR_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cd7f8-93b9-4ee8-b629-f3de587b4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - load the LM negative words into a list called LM_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3bfea-9769-4713-9243-d47f546881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - load the LM positive words into a list called LM_positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcdd66-e646-4a64-bf26-a30092d1d970",
   "metadata": {},
   "source": [
    "## Looping over a dataframe and adding a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8894af2c-986e-4ec5-b642-96ca502f5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row : 0, URL: blahblah.com\n",
      "Row : 1, URL: wikisomething.com\n",
      "Row : 2, URL: wiki.com\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Security</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M</td>\n",
       "      <td>blahblah.com</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLSA</td>\n",
       "      <td>wikisomething.com</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APPL</td>\n",
       "      <td>wiki.com</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Security                URL  Sentiment\n",
       "0       3M       blahblah.com        8.0\n",
       "1     TLSA  wikisomething.com        9.0\n",
       "2     APPL           wiki.com        0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# step 1 will load some database and prep it for the loopy parts\n",
    "# here, we will just use a toy dataset\n",
    "\n",
    "toy_database = pd.DataFrame({\"Security\":['3M','TLSA','APPL'],\n",
    "             \"URL\":['blahblah.com','wikisomething.com','wiki.com']})\n",
    "\n",
    "# step 2: figure out how to loop through this dataframe \n",
    "# yes, an actual for loop on a dataframe (booooooo)\n",
    "for index, row in toy_database.iterrows():\n",
    "    # print the row's index, and the url from the row, this will confirm if we are looping right\n",
    "    print(f\"Row : {index}, URL: {row['URL']}\")\n",
    "\n",
    "    # A. here, you would open the related 10k, but SKIP this for now\n",
    "\n",
    "    # B. You'd measure the sentiment here. Let's just pretend that \n",
    "    # you opened+cleaned+built a sentiment variable \n",
    "    # called \"sentiment_positive\" (a bad name, but this is just example code!)\n",
    "\n",
    "    sentiment_positive = random.randint(0,10) # this is a silly line to \"simulate\" that got a value for this variable\n",
    "\n",
    "    toy_database.at[index, 'Sentiment'] = sentiment_positive\n",
    "\n",
    "toy_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f890f76-b14b-43af-bc30-945d8300d48e",
   "metadata": {},
   "source": [
    "## Measure sentiment\n",
    "\n",
    "What fraction of the words in this \"document\" (sentence) are \"happy\" words?\n",
    "\n",
    "Answer: 2/13. Let's replicate that with code. \n",
    "\n",
    "First, count the length of the document.\n",
    "\n",
    "Then, count how many times each word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57e4fb0-1b06-4406-9127-545cbd3ef723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "I am happy that you are here. I am all smiles.  \t  \n",
      "  So hopeful!\n"
     ]
    }
   ],
   "source": [
    "happy_sentiment = ['happy','smile','hopeful']\n",
    "\n",
    "sentence = 'I am happy that you are here. I am all smiles.  \\t  \\n  So hopeful!'\n",
    "\n",
    "# q0 count the number of \"words\" (the doc length)\n",
    "\n",
    "print(len(sentence.split()))   # nice - works with any whitespace and repeated whitespace \n",
    "\n",
    "print(sentence) # repeated whitespace\n",
    "\n",
    "len(re.findall(\"\\s+\",sentence))+1\n",
    "\n",
    "doc_length = len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70054e4-4ef9-47d3-ae7d-aabff017a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q1 count how many times \"happy\" is used in the doc\n",
    "# hint: https://ledatascifi.github.io/ledatascifi-2024/content/04/02b_regex.html\n",
    "\n",
    "len(re.findall(\"happy\",sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b34b4bfa-b7d7-4d33-ba3b-76b04b041a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q2 count how many times \"smile\" is used in the doc\n",
    "\n",
    "len(re.findall(\"smile\",sentence)) # matches \"smiles\" but I don't want that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78cce9e7-43ad-4f66-9fb1-21912daf72b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\bsmile\n",
      "\\bsmiles\\b\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# fix: \"boundary\" indicator in the pattern \n",
    "# \\b means \"boundary\n",
    "\n",
    "print(len(re.findall(\"\\bsmile\\b\",sentence)))   # 0\n",
    "print(len(re.findall(\"\\bsmiles\\b\",sentence)))   # 0 oops \n",
    "\n",
    "print(\"\\bsmiles\\b\") # \\b is a special character, not slash b\n",
    "\n",
    "print(r\"\\bsmiles\\b\") # r\" \" means raw string \n",
    "\n",
    "print(len(re.findall(r\"\\bsmiles\\b\",sentence)))   # 1 - good!\n",
    "print(len(re.findall(r\"\\bsmile\\b\",sentence)))    # 0 - good!\n",
    "\n",
    "# learned two things:\n",
    "# 1 with a regex, use raw strings!!!! \n",
    "# 2 \\b is a word boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ee35ab-cf0d-4c52-b294-70f5a8c1344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# q3 count how many times \"smile\" or \"happy\" is used in the doc\n",
    "# hint: similar to q2 answer... \n",
    "# the answer is somewhere this page: https://regexone.com/\n",
    "\n",
    "print(len(re.findall(r\"\\b(smile|happy)\\b\",sentence)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd5699-4591-4252-bc6d-ba7a732f9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4 - prof demo - count how many time all the words are in the doc\n",
    "# 4.4.4 has examples + output\n",
    "# docstring: https://github.com/LeDataSciFi/ledatascifi-2024/blob/main/community_codebook/near_regex.py\n",
    "# solve \n",
    "\n",
    "print(\n",
    "len(re.findall(NEAR_regex( ['(smile|happy|hopeful)'  ] ),\n",
    "               sentence))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929ffa9-960e-4716-8b26-eead77ad383f",
   "metadata": {},
   "source": [
    "## the parentheses are important!\n",
    "\n",
    "Contrast these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1be1b0-e2b0-4871-b125-0738712d6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall(NEAR_regex( ['smile|happy|hopeful'  ] ),\n",
    "               sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47d033b7-c8aa-4537-8c6a-cae39b53a87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\b(smile|happy|hopeful)\\\\b)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this func makes the regex\n",
    "# give it a list\n",
    "# elements in the list are a \"regex topic\"\n",
    "NEAR_regex( [ '(smile|happy|hopeful)'  ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5c4412f-a46d-4720-a5fb-cbe1e9be2510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\bsmile|happy|hopeful\\\\b)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEAR_regex( [ 'smile|happy|hopeful'  ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f42fa877-20a1-4f7f-890a-fb55f84b9589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(happy|smile|hopeful)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q5 - using py's string functions, convert\n",
    "# happy_sentiment into the format NEAR_regex() wants \n",
    "# hint: 4.4.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c738e28-d4ad-49ea-8872-3bb3f6fbdabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q6 - calculate the doc's happy_sentiment score\n",
    "\n",
    "happy_regx = \"(\"+ \"|\".join(happy_sentiment) +\")\"\n",
    "\n",
    "len(re.findall(NEAR_regex( [happy_regx ] ), sentence)) / doc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd49572",
   "metadata": {},
   "source": [
    "Anchor phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "727dac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(face|head)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q7: how many times is (happy or smile) near (face or head)? \n",
    "\n",
    "body_parts = ['face','head']\n",
    "body_regx = \"(\"+ \"|\".join(body_parts) +\")\"\n",
    "print(body_regx)\n",
    "\n",
    "sentence1 = 'I see smile on your face. That is so awesome!'\n",
    "sentence2 = 'I see smile. That is so awesome!'\n",
    "\n",
    "# do on sentence1 - \n",
    "# using py's string functions, convert body_parts into the format NEAR_regex()\n",
    "# then use near_regex()\n",
    "print(\n",
    "len(re.findall(NEAR_regex( [happy_regx ,body_regx] ), sentence1))  # 1 good \n",
    ")\n",
    "# do on sentence2\n",
    "\n",
    "len(re.findall(NEAR_regex( [happy_regx ,body_regx] ), sentence2))  # 0 good \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9906de8-c99f-4651-9cfb-2a7ad8f4cc60",
   "metadata": {},
   "source": [
    "## Opening a 10-K file\n",
    "\n",
    "I'm giving everyone this code because dealing with Zips is a headache the first 15 times you do it.\n",
    "- Open the zip before the loop and get a list of all files already in it\n",
    "- With that zip open, do your loopy stuff inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3c63770-1ea8-4e97-96aa-03030e4751b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the zip file (do this before the for loop\n",
    "# so you only open it one time... faster)\n",
    "with ZipFile('10k_files/10k_files_practice.zip','r') as zipfolder:\n",
    "    \n",
    "    # before the loop, get list of files in zipped folder\n",
    "    file_list = zipfolder.namelist()\n",
    "        \n",
    "    # replace this with how you'd loop over the dataframe\n",
    "    # which you already know...\n",
    "    for firm in [1800]: # \n",
    "        \n",
    "        # get a list of possible files for this firm\n",
    "        firm_folder    = f\"sec-edgar-filings/{str(firm).zfill(10)}/10-K/*/*.html\"\n",
    "        possible_files = fnmatch.filter(file_list, firm_folder) \n",
    "        if len(possible_files) == 0: \n",
    "            continue\n",
    "            \n",
    "        fpath = possible_files[0] # the first match is the path to the file\n",
    "\n",
    "        # open the file (this is a little different!)\n",
    "        with zipfolder.open(fpath) as report_file:\n",
    "            html = report_file.read().decode(encoding=\"utf-8\")\n",
    "            \n",
    "        # do more stuff here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9633d1-6074-4f45-a207-ec1e2633ea6f",
   "metadata": {},
   "source": [
    "## Cleaning the html\n",
    "\n",
    "Print out `html`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48b9ce50-afea-4414-8fc1-71499a31aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\\n\\n      <!-- iXBRL document created with: Toppan Merrill Bridge iXBRL 9.6.8042.36810 -->\\n      <!-- Based on: iXBRL 1.1 -->\\n      <!-- Created on: 2/18/2022 12:53:13 AM -->\\n      <!-- iXBRL Library version: 1.0.8042.36816 -->\\n      <!-- iXBRL Service Job ID: f92a8d11-abb5-46dc-a356-1f63ff59b8d5 -->\\n\\n  <html xmlns:us-gaap=\"http://fasb.org/us-gaap/2021-01-31\" xmlns:link=\"http://www.xbrl.org/2003/linkbase\" xmlns:country=\"http://xbrl.sec.gov/country/2021\" xmlns:'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebf63d-588b-4b5c-8834-0d04019230f7",
   "metadata": {},
   "source": [
    "Regex won't work on this as is! We need to remove all the html tags, drop the hidden data, and then, with the remaining text, clean it up using the \"Good ideas\" in 4.4 and 4.4.4 of the book. However, we have to slightly adjust the code. \n",
    "\n",
    "1. Use BeautifulSoup() with the `lxml-xml` parser. Call the output `soup`. Don't use `get_text` yet. \n",
    "1. Delete the hidden XBRL \n",
    "\n",
    "    ```python\n",
    "    for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "        div.decompose()\n",
    "    ```\n",
    "    \n",
    "1. Continue on (get the text from the soup, and continue from there...)\n",
    "2. Check: My cleaned string says ______ in positions ___-___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "426ad006-18ee-4a08-b826-ac8a4c1e61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,features='lxml-xml') \n",
    "\n",
    "for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "    div.decompose()\n",
    "\n",
    "document = soup.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "726ecd73-f201-481c-b9a2-0ad2280f000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = re.sub(r'\\W',' ',document)   # no punctuation\n",
    "document = re.sub(r'\\s+',' ',document)  # excess whtiespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "078ccd9e-9da8-4693-9cb5-2aca554b692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oductsthese products include a broad line of branded generic pharmaceuticals manufactured worldwide and marketed and sold outside the united states in emerging markets these products are generally sold directly to wholesalers distributors government agencies health care facilities pharmacies and independent retailers from abbott owned distribution centers and public warehouses depending on the market served certain products are co marketed or co promoted with or licensed from other companies the principal products included i'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.find('these products include')  # an early sentence in the 10-k, it's at character 53000! \n",
    "\n",
    "document[53470:54000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b70af-c46c-47dd-85f7-31c56c7f0cec",
   "metadata": {},
   "source": [
    "## Get 10-K dates\n",
    "\n",
    "We need to know when the 10-K is released to see the stock returns around it.\n",
    "\n",
    "I'm going to give you most of this code. How I figured it out:\n",
    "- I know we have the CIK and accession number\n",
    "- Looked for EDGAR urls that have CIK + accession number, and then list filing date on the page\n",
    "- https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n",
    "- `requests_html` ([my listed suggestion here](https://ledatascifi.github.io/ledatascifi-2024/content/04/01_Intro_to_scraping.html#my-suggestion)) is the `requests` module for getting data from the web PLUS the ability to grab parts of the html\n",
    "    \n",
    "Exercise:\n",
    "- I used code straight off the [documentation's home page](https://requests.readthedocs.io/projects/requests-html/en/latest/), adapted slightly. Look for examples that _find_ parts of the html.\n",
    "- You'll need to figure out the \"CSS Selector\"\n",
    "    - right click on the filing date on the webpage, click inspect\n",
    "    - in the area that popped up, right click on html code containing that date and copy the CSS selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc3b5286-1c2a-490c-aa1a-179d5cc22700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n"
     ]
    }
   ],
   "source": [
    "# before the loop, set up a browser session\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "\n",
    "# inside your loop, get the cik and accession number for the filing\n",
    "\n",
    "cik = 1122304\n",
    "accession_number = '0001193125-15-118890'\n",
    "\n",
    "# *one* way to get the filing date... \n",
    "\n",
    "url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}-index.html'\n",
    "print(url) # check it out...\n",
    "r = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307f137-fa28-47d4-a45c-ff5b42d9d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: get the filing date out of this \"r\" object (one line of code will do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34a7d5-f345-49df-93c3-13d46ad1a160",
   "metadata": {},
   "source": [
    "To use this in your actual midterm, save the  accession_number to your database while doing the main for loop to parse the text.\n",
    "\n",
    "Then, after that, I wrote a second for loop that loops over the rows, and uses the code above to grab the date. I added some error checking (What if we don't have an accession number for that firm, what if the url is wrong, or the server denies you, or the line of code with filing_date fails?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0af1b2-4d57-448b-8673-3e20d5cb2b50",
   "metadata": {},
   "source": [
    "## Get returns around the 10-K dates\n",
    "\n",
    "[Returns for 2022 are here.](https://github.com/LeDataSciFi/data/blob/main/Stock%20Returns%20(CRSP))\n",
    "\n",
    "Before you try to use that, below is a toy dataset of returns and filing dates that mimic the structure of the data you'll actually have. \n",
    "\n",
    "Goals, in **reverse** order:\n",
    "1. What is the [0,2] and [3,10] cumulative returns for each firm? It's easy to actually figure out! Doing so will help you with the pseudo, and in any case... how can you know if you're right otherwise?\n",
    "2. Make an intermediate dataset with these variables (which is enough to answer goal 1). \n",
    "   - ticker\n",
    "   - date\n",
    "   - ret\n",
    "   - trading_days_since_filing (0 on the filing date or the first trading day after it). This is what the midterm calls for.\n",
    "3. Find the answer manually in excel. `crsp_example.xlsx` is in the handouts folder. This will help you find the steps you need to take. \n",
    "\n",
    "If you figured out the bonus on assignment 4, you're set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ticker': ['JJSF']*20 + ['TSLA']*20,\n",
    "    'date': ['2021-12-01', '2021-12-02', '2021-12-03', '2021-12-06', '2021-12-07', '2021-12-08', '2021-12-09', '2021-12-10', '2021-12-13', '2021-12-14', '2021-12-15', '2021-12-16', '2021-12-17', '2021-12-20', '2021-12-21', '2021-12-22', '2021-12-23', '2021-12-27', '2021-12-28', '2021-12-29'] + ['2022-12-02', '2022-12-05', '2022-12-06', '2022-12-07', '2022-12-08', '2022-12-09', '2022-12-12', '2022-12-13', '2022-12-14', '2022-12-15', '2022-12-16', '2022-12-19', '2022-12-20', '2022-12-21', '2022-12-22', '2022-12-23', '2022-12-27', '2022-12-28', '2022-12-29', '2022-12-30'],\n",
    "    'ret': [-0.011276, 0.030954, 0.000287, 0.014362, 0.012459, 0.017200, -0.010173, 0.011875, 0.012559, 0.002508, 0.022852, 0.012360, 0.017387, -0.008957, 0.016840, -0.000256, -0.002558, 0.009041, -0.002097, 0.010189] + [0.000822, -0.063687, -0.014415, -0.032143, -0.003447, 0.032345, -0.062720, -0.040937, -0.025784, 0.005548, -0.047187, -0.002396, -0.080536, -0.001669, -0.088828, -0.017551, -0.114089, 0.033089, 0.080827, 0.011164]\n",
    "}\n",
    "\n",
    "crsp = pd.DataFrame(data)\n",
    "crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "\n",
    "fake_filings = pd.DataFrame({'ticker':['JJSF','TSLA'],\n",
    "                             'filing_date':['2021-12-03','2022-12-13']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8b74f-0d1c-4e4a-bf3a-9e598f91cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try here...\n",
    "\n",
    "# pseudocode first! imagine the structure of the dataset you want and work backwards, you'll struggle otherwise!\n",
    "\n",
    "# this really is a paper and pencil problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce527-c804-4998-9d6b-ec69d32a0588",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "The readme shows what the output dataset should look like, roughly. The midterm directions elaborate (10 sentiment variables, 2 return measures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6bb1-d32e-4ec9-a797-06c84556436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
